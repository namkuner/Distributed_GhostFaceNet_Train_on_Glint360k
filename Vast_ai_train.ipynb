{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45696f59-5f8d-4aaf-86a4-4837f7f03009",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b7416-1212-443d-8085-4a3befcd804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a01744-9c81-4183-a0da-8ef4c9e4b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Training: 2024-07-20 02:02:31,165-rank_id: 0\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mnamkuner\u001B[0m (\u001B[33mnamkunerr\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mnamkuner\u001B[0m (\u001B[33mnamkunerr\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.17.5\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/workspace/Distributed_GhostFaceNet_Train_on_Glint360k/wandb/run-20240720_020236-889rb4zx\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.17.5\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/workspace/Distributed_GhostFaceNet_Train_on_Glint360k/wandb/run-20240720_020236-icbbxcoi\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33m240720_0202_GPU1\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/namkunerr/GhostFaceNets%20on%20Asian%20and%20MS1MV3%20Dataset\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/namkunerr/GhostFaceNets%20on%20Asian%20and%20MS1MV3%20Dataset/runs/icbbxcoi\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33m240720_0202_GPU0\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/namkunerr/GhostFaceNets%20on%20Asian%20and%20MS1MV3%20Dataset\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/namkunerr/GhostFaceNets%20on%20Asian%20and%20MS1MV3%20Dataset/runs/889rb4zx\u001B[0m\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1772: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1772: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.\n",
      "  warnings.warn(\n",
      "Training: 2024-07-20 02:02:59,622-: margin_list              (1.0, 0.0, 0.4)\n",
      "Training: 2024-07-20 02:02:59,622-: network                  ghostfacenetsv2\n",
      "Training: 2024-07-20 02:02:59,623-: resume                   False\n",
      "Training: 2024-07-20 02:02:59,623-: save_all_states          True\n",
      "Training: 2024-07-20 02:02:59,623-: output                   ms1mv3_asian_arcface_ghostfacenets\n",
      "Training: 2024-07-20 02:02:59,624-: embedding_size           512\n",
      "Training: 2024-07-20 02:02:59,624-: sample_rate              1\n",
      "Training: 2024-07-20 02:02:59,624-: interclass_filtering_threshold0\n",
      "Training: 2024-07-20 02:02:59,624-: fp16                     True\n",
      "Training: 2024-07-20 02:02:59,625-: batch_size               16\n",
      "Training: 2024-07-20 02:02:59,625-: optimizer                sgd\n",
      "Training: 2024-07-20 02:02:59,625-: lr                       0.1\n",
      "Training: 2024-07-20 02:02:59,626-: momentum                 0.9\n",
      "Training: 2024-07-20 02:02:59,626-: weight_decay             0.0005\n",
      "Training: 2024-07-20 02:02:59,626-: verbose                  2000\n",
      "Training: 2024-07-20 02:02:59,626-: frequent                 10\n",
      "Training: 2024-07-20 02:02:59,627-: dali                     False\n",
      "Training: 2024-07-20 02:02:59,627-: dali_aug                 False\n",
      "Training: 2024-07-20 02:02:59,627-: gradient_acc             1\n",
      "Training: 2024-07-20 02:02:59,627-: seed                     2048\n",
      "Training: 2024-07-20 02:02:59,627-: num_workers              2\n",
      "Training: 2024-07-20 02:02:59,627-: wandb_key                daa38a012f1993bc802203d31f828a53c6605938\n",
      "Training: 2024-07-20 02:02:59,627-: suffix_run_name          None\n",
      "Training: 2024-07-20 02:02:59,627-: using_wandb              True\n",
      "Training: 2024-07-20 02:02:59,627-: wandb_entity             namkunerr\n",
      "Training: 2024-07-20 02:02:59,628-: wandb_project            GhostFaceNets on Asian and MS1MV3 Dataset\n",
      "Training: 2024-07-20 02:02:59,628-: wandb_log_all            True\n",
      "Training: 2024-07-20 02:02:59,628-: save_artifacts           True\n",
      "Training: 2024-07-20 02:02:59,628-: wandb_resume             False\n",
      "Training: 2024-07-20 02:02:59,628-: rec                      ms1-mv3-asian-face\n",
      "Training: 2024-07-20 02:02:59,628-: num_classes              6579\n",
      "Training: 2024-07-20 02:02:59,628-: num_image                300000\n",
      "Training: 2024-07-20 02:02:59,628-: num_epoch                20\n",
      "Training: 2024-07-20 02:02:59,628-: warmup_epoch             0\n",
      "Training: 2024-07-20 02:02:59,629-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']\n",
      "Training: 2024-07-20 02:02:59,629-: IMAGE_SIZE               112\n",
      "Training: 2024-07-20 02:02:59,629-: drop_out                 0.1\n",
      "Training: 2024-07-20 02:02:59,629-: width                    1.3\n",
      "Training: 2024-07-20 02:02:59,629-: checkpoint               0\n",
      "Training: 2024-07-20 02:02:59,629-: wandb_id                 bdx6o42y\n",
      "Training: 2024-07-20 02:02:59,629-: notes                    namkuner\n",
      "Training: 2024-07-20 02:02:59,629-: total_batch_size         32\n",
      "Training: 2024-07-20 02:02:59,629-: warmup_step              0\n",
      "Training: 2024-07-20 02:02:59,629-: total_step               187500\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training: 2024-07-20 02:03:24,320-Reducer buckets have been rebuilt in this iteration.\n",
      "Training: 2024-07-20 02:03:32,297-Speed 72.03 samples/sec   Loss 38.6735   LearningRate 0.099989   Epoch: 0   Global Step: 20   Fp16 Grad Scale: 4096   Required: 79 hours\n",
      "Training: 2024-07-20 02:03:36,490-Speed 76.97 samples/sec   Loss 39.2303   LearningRate 0.099984   Epoch: 0   Global Step: 30   Fp16 Grad Scale: 4096   Required: 60 hours\n",
      "Training: 2024-07-20 02:03:40,426-Speed 81.89 samples/sec   Loss 39.3595   LearningRate 0.099979   Epoch: 0   Global Step: 40   Fp16 Grad Scale: 4096   Required: 51 hours\n",
      "Training: 2024-07-20 02:03:44,831-Speed 72.67 samples/sec   Loss 39.3770   LearningRate 0.099973   Epoch: 0   Global Step: 50   Fp16 Grad Scale: 4096   Required: 46 hours\n",
      "Training: 2024-07-20 02:03:49,232-Speed 72.72 samples/sec   Loss 39.5629   LearningRate 0.099968   Epoch: 0   Global Step: 60   Fp16 Grad Scale: 4096   Required: 42 hours\n",
      "Training: 2024-07-20 02:03:53,323-Speed 78.23 samples/sec   Loss 40.0867   LearningRate 0.099963   Epoch: 0   Global Step: 70   Fp16 Grad Scale: 4096   Required: 39 hours\n",
      "Training: 2024-07-20 02:03:57,738-Speed 72.48 samples/sec   Loss 39.5271   LearningRate 0.099957   Epoch: 0   Global Step: 80   Fp16 Grad Scale: 4096   Required: 37 hours\n",
      "Training: 2024-07-20 02:04:01,863-Speed 77.59 samples/sec   Loss 39.6042   LearningRate 0.099952   Epoch: 0   Global Step: 90   Fp16 Grad Scale: 4096   Required: 35 hours\n",
      "Training: 2024-07-20 02:04:06,289-Speed 72.31 samples/sec   Loss 40.0451   LearningRate 0.099947   Epoch: 0   Global Step: 100   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:04:10,528-Speed 75.53 samples/sec   Loss 39.7027   LearningRate 0.099941   Epoch: 0   Global Step: 110   Fp16 Grad Scale: 2048   Required: 33 hours\n",
      "Training: 2024-07-20 02:04:14,933-Speed 72.66 samples/sec   Loss 39.9961   LearningRate 0.099936   Epoch: 0   Global Step: 120   Fp16 Grad Scale: 2048   Required: 32 hours\n",
      "Training: 2024-07-20 02:04:19,080-Speed 77.22 samples/sec   Loss 39.7069   LearningRate 0.099931   Epoch: 0   Global Step: 130   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:04:23,434-Speed 73.51 samples/sec   Loss 39.7173   LearningRate 0.099925   Epoch: 0   Global Step: 140   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:04:27,799-Speed 73.32 samples/sec   Loss 39.8542   LearningRate 0.099920   Epoch: 0   Global Step: 150   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:04:32,111-Speed 74.22 samples/sec   Loss 39.8111   LearningRate 0.099915   Epoch: 0   Global Step: 160   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:04:36,413-Speed 74.39 samples/sec   Loss 39.9827   LearningRate 0.099909   Epoch: 0   Global Step: 170   Fp16 Grad Scale: 2048   Required: 29 hours\n",
      "Training: 2024-07-20 02:04:40,635-Speed 75.90 samples/sec   Loss 39.5736   LearningRate 0.099904   Epoch: 0   Global Step: 180   Fp16 Grad Scale: 2048   Required: 29 hours\n",
      "Training: 2024-07-20 02:04:45,124-Speed 71.30 samples/sec   Loss 39.7562   LearningRate 0.099899   Epoch: 0   Global Step: 190   Fp16 Grad Scale: 2048   Required: 29 hours\n",
      "Training: 2024-07-20 02:04:49,175-Speed 79.01 samples/sec   Loss 39.2225   LearningRate 0.099893   Epoch: 0   Global Step: 200   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:04:53,660-Speed 71.55 samples/sec   Loss 39.9368   LearningRate 0.099888   Epoch: 0   Global Step: 210   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:04:57,748-Speed 78.29 samples/sec   Loss 39.3479   LearningRate 0.099883   Epoch: 0   Global Step: 220   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:05:02,317-Speed 70.05 samples/sec   Loss 40.0559   LearningRate 0.099877   Epoch: 0   Global Step: 230   Fp16 Grad Scale: 4096   Required: 27 hours\n",
      "Training: 2024-07-20 02:05:06,305-Speed 80.24 samples/sec   Loss 40.8532   LearningRate 0.099872   Epoch: 0   Global Step: 240   Fp16 Grad Scale: 4096   Required: 27 hours\n",
      "Training: 2024-07-20 02:05:10,887-Speed 69.93 samples/sec   Loss 41.1246   LearningRate 0.099867   Epoch: 0   Global Step: 250   Fp16 Grad Scale: 4096   Required: 27 hours\n",
      "Training: 2024-07-20 02:05:14,849-Speed 80.80 samples/sec   Loss 41.5416   LearningRate 0.099861   Epoch: 0   Global Step: 260   Fp16 Grad Scale: 4096   Required: 27 hours\n",
      "Training: 2024-07-20 02:05:18,935-Speed 78.35 samples/sec   Loss 39.8178   LearningRate 0.099856   Epoch: 0   Global Step: 270   Fp16 Grad Scale: 4096   Required: 27 hours\n",
      "Training: 2024-07-20 02:05:23,407-Speed 71.56 samples/sec   Loss 40.1226   LearningRate 0.099851   Epoch: 0   Global Step: 280   Fp16 Grad Scale: 4096   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:27,371-Speed 80.82 samples/sec   Loss 40.1157   LearningRate 0.099845   Epoch: 0   Global Step: 290   Fp16 Grad Scale: 4096   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:31,537-Speed 76.82 samples/sec   Loss 40.5415   LearningRate 0.099840   Epoch: 0   Global Step: 300   Fp16 Grad Scale: 4096   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:35,847-Speed 74.26 samples/sec   Loss 40.4169   LearningRate 0.099835   Epoch: 0   Global Step: 310   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:39,896-Speed 79.06 samples/sec   Loss 40.2784   LearningRate 0.099829   Epoch: 0   Global Step: 320   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:44,225-Speed 73.96 samples/sec   Loss 39.9409   LearningRate 0.099824   Epoch: 0   Global Step: 330   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:48,555-Speed 74.05 samples/sec   Loss 39.7738   LearningRate 0.099819   Epoch: 0   Global Step: 340   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:53,808-Speed 60.92 samples/sec   Loss 39.7966   LearningRate 0.099813   Epoch: 0   Global Step: 350   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:05:58,334-Speed 70.70 samples/sec   Loss 39.5950   LearningRate 0.099808   Epoch: 0   Global Step: 360   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:06:02,688-Speed 73.51 samples/sec   Loss 39.9920   LearningRate 0.099803   Epoch: 0   Global Step: 370   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:06:06,749-Speed 78.81 samples/sec   Loss 40.7004   LearningRate 0.099797   Epoch: 0   Global Step: 380   Fp16 Grad Scale: 8192   Required: 26 hours\n",
      "Training: 2024-07-20 02:06:11,216-Speed 71.65 samples/sec   Loss 41.6168   LearningRate 0.099792   Epoch: 0   Global Step: 390   Fp16 Grad Scale: 8192   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:15,325-Speed 77.89 samples/sec   Loss 41.2731   LearningRate 0.099787   Epoch: 0   Global Step: 400   Fp16 Grad Scale: 8192   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:19,469-Speed 77.25 samples/sec   Loss 41.7370   LearningRate 0.099781   Epoch: 0   Global Step: 410   Fp16 Grad Scale: 16384   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:23,796-Speed 73.96 samples/sec   Loss 41.4294   LearningRate 0.099776   Epoch: 0   Global Step: 420   Fp16 Grad Scale: 16384   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:27,838-Speed 79.17 samples/sec   Loss 40.9160   LearningRate 0.099771   Epoch: 0   Global Step: 430   Fp16 Grad Scale: 16384   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:32,226-Speed 72.94 samples/sec   Loss 41.2035   LearningRate 0.099765   Epoch: 0   Global Step: 440   Fp16 Grad Scale: 16384   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:36,232-Speed 79.91 samples/sec   Loss 41.2255   LearningRate 0.099760   Epoch: 0   Global Step: 450   Fp16 Grad Scale: 16384   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:40,236-Speed 79.93 samples/sec   Loss 40.9866   LearningRate 0.099755   Epoch: 0   Global Step: 460   Fp16 Grad Scale: 16384   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:44,637-Speed 72.72 samples/sec   Loss 41.8715   LearningRate 0.099749   Epoch: 0   Global Step: 470   Fp16 Grad Scale: 8192   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:48,612-Speed 80.53 samples/sec   Loss 40.9719   LearningRate 0.099744   Epoch: 0   Global Step: 480   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:52,702-Speed 78.25 samples/sec   Loss 41.3153   LearningRate 0.099739   Epoch: 0   Global Step: 490   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:06:57,153-Speed 71.90 samples/sec   Loss 41.2913   LearningRate 0.099733   Epoch: 0   Global Step: 500   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:07:02,360-Speed 61.47 samples/sec   Loss 41.0991   LearningRate 0.099728   Epoch: 0   Global Step: 510   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:07:06,835-Speed 71.52 samples/sec   Loss 41.5669   LearningRate 0.099723   Epoch: 0   Global Step: 520   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:07:11,005-Speed 76.74 samples/sec   Loss 41.3269   LearningRate 0.099717   Epoch: 0   Global Step: 530   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:07:15,199-Speed 76.32 samples/sec   Loss 40.7661   LearningRate 0.099712   Epoch: 0   Global Step: 540   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:19,742-Speed 70.46 samples/sec   Loss 40.9685   LearningRate 0.099707   Epoch: 0   Global Step: 550   Fp16 Grad Scale: 4096   Required: 25 hours\n",
      "Training: 2024-07-20 02:07:23,903-Speed 76.99 samples/sec   Loss 41.2147   LearningRate 0.099701   Epoch: 0   Global Step: 560   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:28,605-Speed 68.10 samples/sec   Loss 40.5970   LearningRate 0.099696   Epoch: 0   Global Step: 570   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:32,622-Speed 79.75 samples/sec   Loss 40.6997   LearningRate 0.099691   Epoch: 0   Global Step: 580   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:37,333-Speed 67.94 samples/sec   Loss 40.6281   LearningRate 0.099685   Epoch: 0   Global Step: 590   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:41,588-Speed 75.22 samples/sec   Loss 40.8168   LearningRate 0.099680   Epoch: 0   Global Step: 600   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:45,692-Speed 78.00 samples/sec   Loss 41.1849   LearningRate 0.099675   Epoch: 0   Global Step: 610   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:50,009-Speed 74.14 samples/sec   Loss 40.2541   LearningRate 0.099669   Epoch: 0   Global Step: 620   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:54,114-Speed 78.04 samples/sec   Loss 40.6404   LearningRate 0.099664   Epoch: 0   Global Step: 630   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:07:59,725-Speed 57.03 samples/sec   Loss 40.8782   LearningRate 0.099659   Epoch: 0   Global Step: 640   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:03,925-Speed 76.28 samples/sec   Loss 40.4906   LearningRate 0.099653   Epoch: 0   Global Step: 650   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:08,304-Speed 73.09 samples/sec   Loss 40.1855   LearningRate 0.099648   Epoch: 0   Global Step: 660   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:12,419-Speed 77.76 samples/sec   Loss 40.5731   LearningRate 0.099643   Epoch: 0   Global Step: 670   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:16,720-Speed 74.42 samples/sec   Loss 39.6701   LearningRate 0.099637   Epoch: 0   Global Step: 680   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:21,335-Speed 69.36 samples/sec   Loss 40.5936   LearningRate 0.099632   Epoch: 0   Global Step: 690   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:25,816-Speed 71.42 samples/sec   Loss 40.1246   LearningRate 0.099627   Epoch: 0   Global Step: 700   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:29,991-Speed 76.68 samples/sec   Loss 40.1621   LearningRate 0.099621   Epoch: 0   Global Step: 710   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:33,976-Speed 80.37 samples/sec   Loss 40.1114   LearningRate 0.099616   Epoch: 0   Global Step: 720   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:38,334-Speed 73.45 samples/sec   Loss 39.7346   LearningRate 0.099611   Epoch: 0   Global Step: 730   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:42,237-Speed 81.99 samples/sec   Loss 39.7509   LearningRate 0.099605   Epoch: 0   Global Step: 740   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:46,638-Speed 72.71 samples/sec   Loss 39.8711   LearningRate 0.099600   Epoch: 0   Global Step: 750   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:50,803-Speed 76.85 samples/sec   Loss 40.1642   LearningRate 0.099595   Epoch: 0   Global Step: 760   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:55,119-Speed 74.21 samples/sec   Loss 39.4265   LearningRate 0.099589   Epoch: 0   Global Step: 770   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:08:59,134-Speed 79.70 samples/sec   Loss 41.2154   LearningRate 0.099584   Epoch: 0   Global Step: 780   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:03,592-Speed 71.80 samples/sec   Loss 39.6482   LearningRate 0.099579   Epoch: 0   Global Step: 790   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:07,587-Speed 80.12 samples/sec   Loss 40.0780   LearningRate 0.099573   Epoch: 0   Global Step: 800   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:11,852-Speed 75.05 samples/sec   Loss 40.4084   LearningRate 0.099568   Epoch: 0   Global Step: 810   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:16,029-Speed 76.62 samples/sec   Loss 40.0358   LearningRate 0.099563   Epoch: 0   Global Step: 820   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:20,153-Speed 77.61 samples/sec   Loss 39.2117   LearningRate 0.099557   Epoch: 0   Global Step: 830   Fp16 Grad Scale: 8192   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:24,361-Speed 76.07 samples/sec   Loss 39.2947   LearningRate 0.099552   Epoch: 0   Global Step: 840   Fp16 Grad Scale: 16384   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:28,300-Speed 81.28 samples/sec   Loss 39.4108   LearningRate 0.099547   Epoch: 0   Global Step: 850   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:32,787-Speed 71.32 samples/sec   Loss 41.1147   LearningRate 0.099541   Epoch: 0   Global Step: 860   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:36,911-Speed 77.63 samples/sec   Loss 40.0031   LearningRate 0.099536   Epoch: 0   Global Step: 870   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:41,233-Speed 74.04 samples/sec   Loss 40.6678   LearningRate 0.099531   Epoch: 0   Global Step: 880   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:45,229-Speed 80.09 samples/sec   Loss 40.2123   LearningRate 0.099525   Epoch: 0   Global Step: 890   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:49,245-Speed 79.69 samples/sec   Loss 40.0181   LearningRate 0.099520   Epoch: 0   Global Step: 900   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:09:53,507-Speed 75.11 samples/sec   Loss 39.4920   LearningRate 0.099515   Epoch: 0   Global Step: 910   Fp16 Grad Scale: 4096   Required: 23 hours\n",
      "Training: 2024-07-20 02:09:57,713-Speed 76.10 samples/sec   Loss 39.6610   LearningRate 0.099509   Epoch: 0   Global Step: 920   Fp16 Grad Scale: 4096   Required: 24 hours\n",
      "Training: 2024-07-20 02:10:02,066-Speed 73.53 samples/sec   Loss 39.7636   LearningRate 0.099504   Epoch: 0   Global Step: 930   Fp16 Grad Scale: 4096   Required: 23 hours\n",
      "Training: 2024-07-20 02:10:06,554-Speed 71.41 samples/sec   Loss 40.9486   LearningRate 0.099499   Epoch: 0   Global Step: 940   Fp16 Grad Scale: 4096   Required: 23 hours\n",
      "Training: 2024-07-20 02:10:11,120-Speed 70.09 samples/sec   Loss 40.1215   LearningRate 0.099493   Epoch: 0   Global Step: 950   Fp16 Grad Scale: 8192   Required: 23 hours\n",
      "Training: 2024-07-20 02:10:15,220-Speed 78.06 samples/sec   Loss 39.3811   LearningRate 0.099488   Epoch: 0   Global Step: 960   Fp16 Grad Scale: 8192   Required: 23 hours\n",
      "Training: 2024-07-20 02:11:44,289-Speed 3.59 samples/sec   Loss 39.9260   LearningRate 0.099483   Epoch: 0   Global Step: 970   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:13:40,429-Speed 2.76 samples/sec   Loss 39.3825   LearningRate 0.099477   Epoch: 0   Global Step: 980   Fp16 Grad Scale: 8192   Required: 34 hours\n",
      "Training: 2024-07-20 02:14:31,191-Speed 6.30 samples/sec   Loss 38.6639   LearningRate 0.099472   Epoch: 0   Global Step: 990   Fp16 Grad Scale: 8192   Required: 36 hours\n",
      "Training: 2024-07-20 02:14:35,120-Speed 81.48 samples/sec   Loss 40.1488   LearningRate 0.099467   Epoch: 0   Global Step: 1000   Fp16 Grad Scale: 8192   Required: 36 hours\n",
      "Training: 2024-07-20 02:14:39,241-Speed 77.66 samples/sec   Loss 38.8744   LearningRate 0.099461   Epoch: 0   Global Step: 1010   Fp16 Grad Scale: 8192   Required: 36 hours\n",
      "Training: 2024-07-20 02:14:43,133-Speed 82.23 samples/sec   Loss 39.3523   LearningRate 0.099456   Epoch: 0   Global Step: 1020   Fp16 Grad Scale: 8192   Required: 36 hours\n",
      "Training: 2024-07-20 02:14:47,332-Speed 76.22 samples/sec   Loss 39.1365   LearningRate 0.099451   Epoch: 0   Global Step: 1030   Fp16 Grad Scale: 8192   Required: 36 hours\n",
      "Training: 2024-07-20 02:14:51,137-Speed 84.10 samples/sec   Loss 39.3957   LearningRate 0.099445   Epoch: 0   Global Step: 1040   Fp16 Grad Scale: 8192   Required: 35 hours\n",
      "Training: 2024-07-20 02:14:55,141-Speed 79.94 samples/sec   Loss 40.0154   LearningRate 0.099440   Epoch: 0   Global Step: 1050   Fp16 Grad Scale: 16384   Required: 35 hours\n",
      "Training: 2024-07-20 02:14:59,408-Speed 75.10 samples/sec   Loss 39.9262   LearningRate 0.099435   Epoch: 0   Global Step: 1060   Fp16 Grad Scale: 16384   Required: 35 hours\n",
      "Training: 2024-07-20 02:15:03,849-Speed 72.06 samples/sec   Loss 39.7984   LearningRate 0.099429   Epoch: 0   Global Step: 1070   Fp16 Grad Scale: 16384   Required: 35 hours\n",
      "Training: 2024-07-20 02:15:07,896-Speed 79.30 samples/sec   Loss 39.2224   LearningRate 0.099424   Epoch: 0   Global Step: 1080   Fp16 Grad Scale: 16384   Required: 35 hours\n",
      "Training: 2024-07-20 02:15:12,047-Speed 77.27 samples/sec   Loss 39.4216   LearningRate 0.099419   Epoch: 0   Global Step: 1090   Fp16 Grad Scale: 4096   Required: 35 hours\n",
      "Training: 2024-07-20 02:15:16,157-Speed 77.86 samples/sec   Loss 39.5878   LearningRate 0.099413   Epoch: 0   Global Step: 1100   Fp16 Grad Scale: 4096   Required: 35 hours\n",
      "Training: 2024-07-20 02:15:20,506-Speed 73.69 samples/sec   Loss 39.5197   LearningRate 0.099408   Epoch: 0   Global Step: 1110   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:24,409-Speed 82.01 samples/sec   Loss 39.9613   LearningRate 0.099403   Epoch: 0   Global Step: 1120   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:28,430-Speed 79.72 samples/sec   Loss 39.1532   LearningRate 0.099397   Epoch: 0   Global Step: 1130   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:32,794-Speed 73.38 samples/sec   Loss 38.8195   LearningRate 0.099392   Epoch: 0   Global Step: 1140   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:36,743-Speed 81.05 samples/sec   Loss 39.2606   LearningRate 0.099387   Epoch: 0   Global Step: 1150   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:41,045-Speed 74.39 samples/sec   Loss 38.8621   LearningRate 0.099381   Epoch: 0   Global Step: 1160   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:47,027-Speed 53.50 samples/sec   Loss 39.3970   LearningRate 0.099376   Epoch: 0   Global Step: 1170   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:53,123-Speed 52.50 samples/sec   Loss 38.9022   LearningRate 0.099371   Epoch: 0   Global Step: 1180   Fp16 Grad Scale: 4096   Required: 34 hours\n",
      "Training: 2024-07-20 02:15:58,587-Speed 58.57 samples/sec   Loss 38.9599   LearningRate 0.099365   Epoch: 0   Global Step: 1190   Fp16 Grad Scale: 8192   Required: 34 hours\n",
      "Training: 2024-07-20 02:16:02,696-Speed 77.89 samples/sec   Loss 39.3379   LearningRate 0.099360   Epoch: 0   Global Step: 1200   Fp16 Grad Scale: 8192   Required: 34 hours\n",
      "Training: 2024-07-20 02:16:07,128-Speed 72.21 samples/sec   Loss 39.4058   LearningRate 0.099355   Epoch: 0   Global Step: 1210   Fp16 Grad Scale: 8192   Required: 34 hours\n",
      "Training: 2024-07-20 02:16:11,337-Speed 76.03 samples/sec   Loss 40.2892   LearningRate 0.099349   Epoch: 0   Global Step: 1220   Fp16 Grad Scale: 8192   Required: 34 hours\n",
      "Training: 2024-07-20 02:16:15,579-Speed 75.45 samples/sec   Loss 38.6536   LearningRate 0.099344   Epoch: 0   Global Step: 1230   Fp16 Grad Scale: 8192   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:19,903-Speed 74.23 samples/sec   Loss 38.9164   LearningRate 0.099339   Epoch: 0   Global Step: 1240   Fp16 Grad Scale: 8192   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:23,906-Speed 80.00 samples/sec   Loss 39.0514   LearningRate 0.099333   Epoch: 0   Global Step: 1250   Fp16 Grad Scale: 8192   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:28,217-Speed 74.24 samples/sec   Loss 39.7981   LearningRate 0.099328   Epoch: 0   Global Step: 1260   Fp16 Grad Scale: 8192   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:32,209-Speed 80.19 samples/sec   Loss 39.1669   LearningRate 0.099323   Epoch: 0   Global Step: 1270   Fp16 Grad Scale: 8192   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:36,158-Speed 81.21 samples/sec   Loss 40.0866   LearningRate 0.099317   Epoch: 0   Global Step: 1280   Fp16 Grad Scale: 8192   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:40,323-Speed 76.86 samples/sec   Loss 38.5648   LearningRate 0.099312   Epoch: 0   Global Step: 1290   Fp16 Grad Scale: 16384   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:44,717-Speed 72.84 samples/sec   Loss 39.1346   LearningRate 0.099307   Epoch: 0   Global Step: 1300   Fp16 Grad Scale: 16384   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:48,724-Speed 79.87 samples/sec   Loss 39.4907   LearningRate 0.099301   Epoch: 0   Global Step: 1310   Fp16 Grad Scale: 16384   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:52,989-Speed 75.05 samples/sec   Loss 38.4469   LearningRate 0.099296   Epoch: 0   Global Step: 1320   Fp16 Grad Scale: 16384   Required: 33 hours\n",
      "Training: 2024-07-20 02:16:57,174-Speed 76.48 samples/sec   Loss 39.8720   LearningRate 0.099291   Epoch: 0   Global Step: 1330   Fp16 Grad Scale: 16384   Required: 33 hours\n",
      "Training: 2024-07-20 02:17:01,262-Speed 78.28 samples/sec   Loss 39.1095   LearningRate 0.099285   Epoch: 0   Global Step: 1340   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:05,492-Speed 75.74 samples/sec   Loss 38.6017   LearningRate 0.099280   Epoch: 0   Global Step: 1350   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:09,756-Speed 75.07 samples/sec   Loss 39.3191   LearningRate 0.099275   Epoch: 0   Global Step: 1360   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:14,218-Speed 71.73 samples/sec   Loss 38.7175   LearningRate 0.099269   Epoch: 0   Global Step: 1370   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:18,388-Speed 76.82 samples/sec   Loss 39.1697   LearningRate 0.099264   Epoch: 0   Global Step: 1380   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:22,930-Speed 70.47 samples/sec   Loss 38.7369   LearningRate 0.099259   Epoch: 0   Global Step: 1390   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:26,887-Speed 81.90 samples/sec   Loss 38.8914   LearningRate 0.099253   Epoch: 0   Global Step: 1400   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:36,096-Speed 34.75 samples/sec   Loss 39.5416   LearningRate 0.099248   Epoch: 0   Global Step: 1410   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:46,009-Speed 32.29 samples/sec   Loss 39.0946   LearningRate 0.099243   Epoch: 0   Global Step: 1420   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:17:56,029-Speed 31.94 samples/sec   Loss 40.2084   LearningRate 0.099237   Epoch: 0   Global Step: 1430   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:02,212-Speed 51.76 samples/sec   Loss 38.2023   LearningRate 0.099232   Epoch: 0   Global Step: 1440   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:06,137-Speed 81.55 samples/sec   Loss 38.8455   LearningRate 0.099227   Epoch: 0   Global Step: 1450   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:12,506-Speed 50.29 samples/sec   Loss 39.7636   LearningRate 0.099221   Epoch: 0   Global Step: 1460   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:17,452-Speed 64.71 samples/sec   Loss 38.8194   LearningRate 0.099216   Epoch: 0   Global Step: 1470   Fp16 Grad Scale: 8192   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:21,762-Speed 74.27 samples/sec   Loss 38.5037   LearningRate 0.099211   Epoch: 0   Global Step: 1480   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:25,991-Speed 75.68 samples/sec   Loss 40.8707   LearningRate 0.099205   Epoch: 0   Global Step: 1490   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:30,138-Speed 77.17 samples/sec   Loss 38.7388   LearningRate 0.099200   Epoch: 0   Global Step: 1500   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:34,087-Speed 81.06 samples/sec   Loss 39.2162   LearningRate 0.099195   Epoch: 0   Global Step: 1510   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:38,539-Speed 71.89 samples/sec   Loss 39.0699   LearningRate 0.099189   Epoch: 0   Global Step: 1520   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:42,821-Speed 74.73 samples/sec   Loss 39.2090   LearningRate 0.099184   Epoch: 0   Global Step: 1530   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:46,909-Speed 78.30 samples/sec   Loss 39.7085   LearningRate 0.099179   Epoch: 0   Global Step: 1540   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:50,910-Speed 80.06 samples/sec   Loss 39.6200   LearningRate 0.099173   Epoch: 0   Global Step: 1550   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:55,220-Speed 74.42 samples/sec   Loss 38.5672   LearningRate 0.099168   Epoch: 0   Global Step: 1560   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:18:59,726-Speed 71.02 samples/sec   Loss 39.1688   LearningRate 0.099163   Epoch: 0   Global Step: 1570   Fp16 Grad Scale: 16384   Required: 32 hours\n",
      "Training: 2024-07-20 02:19:04,063-Speed 73.80 samples/sec   Loss 39.6610   LearningRate 0.099157   Epoch: 0   Global Step: 1580   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:08,467-Speed 72.69 samples/sec   Loss 38.8959   LearningRate 0.099152   Epoch: 0   Global Step: 1590   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:12,440-Speed 80.54 samples/sec   Loss 38.4423   LearningRate 0.099147   Epoch: 0   Global Step: 1600   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:16,759-Speed 74.24 samples/sec   Loss 39.2713   LearningRate 0.099141   Epoch: 0   Global Step: 1610   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:20,939-Speed 76.58 samples/sec   Loss 38.4668   LearningRate 0.099136   Epoch: 0   Global Step: 1620   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:25,026-Speed 78.33 samples/sec   Loss 39.0768   LearningRate 0.099131   Epoch: 0   Global Step: 1630   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:29,464-Speed 72.12 samples/sec   Loss 38.0797   LearningRate 0.099125   Epoch: 0   Global Step: 1640   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:33,370-Speed 81.99 samples/sec   Loss 39.3682   LearningRate 0.099120   Epoch: 0   Global Step: 1650   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:37,812-Speed 72.04 samples/sec   Loss 40.0761   LearningRate 0.099115   Epoch: 0   Global Step: 1660   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:41,992-Speed 76.58 samples/sec   Loss 38.7125   LearningRate 0.099109   Epoch: 0   Global Step: 1670   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:46,123-Speed 77.54 samples/sec   Loss 39.7778   LearningRate 0.099104   Epoch: 0   Global Step: 1680   Fp16 Grad Scale: 16384   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:50,124-Speed 80.00 samples/sec   Loss 39.9398   LearningRate 0.099099   Epoch: 0   Global Step: 1690   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:54,521-Speed 72.79 samples/sec   Loss 39.0780   LearningRate 0.099093   Epoch: 0   Global Step: 1700   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:19:58,751-Speed 75.65 samples/sec   Loss 39.3382   LearningRate 0.099088   Epoch: 0   Global Step: 1710   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:20:03,170-Speed 72.43 samples/sec   Loss 40.1001   LearningRate 0.099083   Epoch: 0   Global Step: 1720   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:20:07,589-Speed 72.43 samples/sec   Loss 38.5513   LearningRate 0.099077   Epoch: 0   Global Step: 1730   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:20:11,633-Speed 79.29 samples/sec   Loss 38.4824   LearningRate 0.099072   Epoch: 0   Global Step: 1740   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:20:15,713-Speed 78.44 samples/sec   Loss 42.5489   LearningRate 0.099067   Epoch: 0   Global Step: 1750   Fp16 Grad Scale: 2048   Required: 31 hours\n",
      "Training: 2024-07-20 02:20:20,002-Speed 74.72 samples/sec   Loss 38.7611   LearningRate 0.099061   Epoch: 0   Global Step: 1760   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:24,101-Speed 78.17 samples/sec   Loss 38.9018   LearningRate 0.099056   Epoch: 0   Global Step: 1770   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:28,390-Speed 74.63 samples/sec   Loss 39.8601   LearningRate 0.099051   Epoch: 0   Global Step: 1780   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:32,339-Speed 81.04 samples/sec   Loss 38.6233   LearningRate 0.099045   Epoch: 0   Global Step: 1790   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:36,739-Speed 72.75 samples/sec   Loss 38.9081   LearningRate 0.099040   Epoch: 0   Global Step: 1800   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:40,704-Speed 80.79 samples/sec   Loss 39.4063   LearningRate 0.099035   Epoch: 0   Global Step: 1810   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:44,916-Speed 75.99 samples/sec   Loss 40.1056   LearningRate 0.099029   Epoch: 0   Global Step: 1820   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:48,812-Speed 82.16 samples/sec   Loss 39.2820   LearningRate 0.099024   Epoch: 0   Global Step: 1830   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:53,136-Speed 74.01 samples/sec   Loss 38.8778   LearningRate 0.099019   Epoch: 0   Global Step: 1840   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:20:57,122-Speed 80.30 samples/sec   Loss 40.6313   LearningRate 0.099013   Epoch: 0   Global Step: 1850   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:01,107-Speed 80.33 samples/sec   Loss 38.7014   LearningRate 0.099008   Epoch: 0   Global Step: 1860   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:05,431-Speed 74.03 samples/sec   Loss 39.8719   LearningRate 0.099003   Epoch: 0   Global Step: 1870   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:09,796-Speed 73.31 samples/sec   Loss 39.4175   LearningRate 0.098997   Epoch: 0   Global Step: 1880   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:14,113-Speed 74.13 samples/sec   Loss 39.3077   LearningRate 0.098992   Epoch: 0   Global Step: 1890   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:18,034-Speed 81.64 samples/sec   Loss 40.5202   LearningRate 0.098987   Epoch: 0   Global Step: 1900   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:22,389-Speed 73.49 samples/sec   Loss 39.6834   LearningRate 0.098981   Epoch: 0   Global Step: 1910   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:26,530-Speed 77.29 samples/sec   Loss 39.9139   LearningRate 0.098976   Epoch: 0   Global Step: 1920   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:30,458-Speed 81.59 samples/sec   Loss 38.5980   LearningRate 0.098971   Epoch: 0   Global Step: 1930   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:34,872-Speed 72.52 samples/sec   Loss 39.3398   LearningRate 0.098965   Epoch: 0   Global Step: 1940   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:38,934-Speed 78.81 samples/sec   Loss 39.1363   LearningRate 0.098960   Epoch: 0   Global Step: 1950   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:43,090-Speed 77.05 samples/sec   Loss 41.0416   LearningRate 0.098955   Epoch: 0   Global Step: 1960   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:21:47,002-Speed 81.83 samples/sec   Loss 39.5924   LearningRate 0.098949   Epoch: 0   Global Step: 1970   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:21:51,360-Speed 73.51 samples/sec   Loss 39.6940   LearningRate 0.098944   Epoch: 0   Global Step: 1980   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:21:55,617-Speed 75.19 samples/sec   Loss 39.6039   LearningRate 0.098939   Epoch: 0   Global Step: 1990   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:21:59,932-Speed 74.17 samples/sec   Loss 39.8551   LearningRate 0.098933   Epoch: 0   Global Step: 2000   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:04,112-Speed 76.57 samples/sec   Loss 39.6221   LearningRate 0.098928   Epoch: 0   Global Step: 2010   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:08,431-Speed 74.14 samples/sec   Loss 38.7265   LearningRate 0.098923   Epoch: 0   Global Step: 2020   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:12,647-Speed 75.92 samples/sec   Loss 39.9829   LearningRate 0.098917   Epoch: 0   Global Step: 2030   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:16,667-Speed 79.63 samples/sec   Loss 39.5003   LearningRate 0.098912   Epoch: 0   Global Step: 2040   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:20,994-Speed 74.04 samples/sec   Loss 39.2376   LearningRate 0.098907   Epoch: 0   Global Step: 2050   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:25,042-Speed 79.15 samples/sec   Loss 39.7246   LearningRate 0.098901   Epoch: 0   Global Step: 2060   Fp16 Grad Scale: 4096   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:29,041-Speed 80.11 samples/sec   Loss 38.7739   LearningRate 0.098896   Epoch: 0   Global Step: 2070   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:33,419-Speed 73.11 samples/sec   Loss 39.9665   LearningRate 0.098891   Epoch: 0   Global Step: 2080   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:37,463-Speed 79.15 samples/sec   Loss 40.1853   LearningRate 0.098885   Epoch: 0   Global Step: 2090   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:41,635-Speed 76.73 samples/sec   Loss 40.1152   LearningRate 0.098880   Epoch: 0   Global Step: 2100   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:45,602-Speed 80.76 samples/sec   Loss 39.1030   LearningRate 0.098875   Epoch: 0   Global Step: 2110   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:49,868-Speed 75.07 samples/sec   Loss 39.8096   LearningRate 0.098869   Epoch: 0   Global Step: 2120   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:54,235-Speed 73.30 samples/sec   Loss 38.7023   LearningRate 0.098864   Epoch: 0   Global Step: 2130   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:22:58,237-Speed 79.96 samples/sec   Loss 39.1958   LearningRate 0.098859   Epoch: 0   Global Step: 2140   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:02,115-Speed 82.54 samples/sec   Loss 39.5086   LearningRate 0.098853   Epoch: 0   Global Step: 2150   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:06,342-Speed 75.74 samples/sec   Loss 39.6966   LearningRate 0.098848   Epoch: 0   Global Step: 2160   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:10,487-Speed 77.21 samples/sec   Loss 39.7580   LearningRate 0.098843   Epoch: 0   Global Step: 2170   Fp16 Grad Scale: 16384   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:14,610-Speed 77.63 samples/sec   Loss 39.0876   LearningRate 0.098837   Epoch: 0   Global Step: 2180   Fp16 Grad Scale: 16384   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:18,831-Speed 75.82 samples/sec   Loss 39.5508   LearningRate 0.098832   Epoch: 0   Global Step: 2190   Fp16 Grad Scale: 16384   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:22,751-Speed 81.80 samples/sec   Loss 38.8878   LearningRate 0.098827   Epoch: 0   Global Step: 2200   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:27,258-Speed 71.00 samples/sec   Loss 39.3010   LearningRate 0.098821   Epoch: 0   Global Step: 2210   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:31,240-Speed 80.47 samples/sec   Loss 39.4361   LearningRate 0.098816   Epoch: 0   Global Step: 2220   Fp16 Grad Scale: 8192   Required: 29 hours\n",
      "Training: 2024-07-20 02:23:35,236-Speed 80.09 samples/sec   Loss 41.0784   LearningRate 0.098811   Epoch: 0   Global Step: 2230   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:23:39,474-Speed 75.51 samples/sec   Loss 43.0118   LearningRate 0.098805   Epoch: 0   Global Step: 2240   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:23:43,636-Speed 76.98 samples/sec   Loss 39.1834   LearningRate 0.098800   Epoch: 0   Global Step: 2250   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:23:48,120-Speed 71.38 samples/sec   Loss 39.8290   LearningRate 0.098795   Epoch: 0   Global Step: 2260   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:23:52,338-Speed 75.88 samples/sec   Loss 39.2204   LearningRate 0.098789   Epoch: 0   Global Step: 2270   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:23:56,568-Speed 75.66 samples/sec   Loss 41.4722   LearningRate 0.098784   Epoch: 0   Global Step: 2280   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:00,451-Speed 82.44 samples/sec   Loss 44.9024   LearningRate 0.098779   Epoch: 0   Global Step: 2290   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:05,591-Speed 62.26 samples/sec   Loss 44.1423   LearningRate 0.098773   Epoch: 0   Global Step: 2300   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:11,191-Speed 57.16 samples/sec   Loss 39.9950   LearningRate 0.098768   Epoch: 0   Global Step: 2310   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:15,903-Speed 67.93 samples/sec   Loss 41.0894   LearningRate 0.098763   Epoch: 0   Global Step: 2320   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:20,658-Speed 67.31 samples/sec   Loss 40.4805   LearningRate 0.098757   Epoch: 0   Global Step: 2330   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:25,242-Speed 69.85 samples/sec   Loss 39.9541   LearningRate 0.098752   Epoch: 0   Global Step: 2340   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:29,429-Speed 76.49 samples/sec   Loss 42.1201   LearningRate 0.098747   Epoch: 0   Global Step: 2350   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:33,802-Speed 73.20 samples/sec   Loss 40.5217   LearningRate 0.098741   Epoch: 0   Global Step: 2360   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:38,037-Speed 75.58 samples/sec   Loss 39.5300   LearningRate 0.098736   Epoch: 0   Global Step: 2370   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:41,954-Speed 81.78 samples/sec   Loss 39.2114   LearningRate 0.098731   Epoch: 0   Global Step: 2380   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:45,799-Speed 83.25 samples/sec   Loss 39.2491   LearningRate 0.098725   Epoch: 0   Global Step: 2390   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:49,624-Speed 83.69 samples/sec   Loss 39.4571   LearningRate 0.098720   Epoch: 0   Global Step: 2400   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:53,734-Speed 77.86 samples/sec   Loss 40.6917   LearningRate 0.098715   Epoch: 0   Global Step: 2410   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:24:58,652-Speed 65.08 samples/sec   Loss 39.5032   LearningRate 0.098709   Epoch: 0   Global Step: 2420   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:03,004-Speed 73.54 samples/sec   Loss 39.2889   LearningRate 0.098704   Epoch: 0   Global Step: 2430   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:07,499-Speed 71.21 samples/sec   Loss 40.2204   LearningRate 0.098699   Epoch: 0   Global Step: 2440   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:11,439-Speed 81.22 samples/sec   Loss 39.3316   LearningRate 0.098693   Epoch: 0   Global Step: 2450   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:15,818-Speed 73.09 samples/sec   Loss 39.9688   LearningRate 0.098688   Epoch: 0   Global Step: 2460   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:20,139-Speed 74.08 samples/sec   Loss 39.4216   LearningRate 0.098683   Epoch: 0   Global Step: 2470   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:24,121-Speed 80.38 samples/sec   Loss 40.2638   LearningRate 0.098677   Epoch: 0   Global Step: 2480   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:28,187-Speed 79.25 samples/sec   Loss 39.5482   LearningRate 0.098672   Epoch: 0   Global Step: 2490   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:32,411-Speed 75.77 samples/sec   Loss 40.0046   LearningRate 0.098667   Epoch: 0   Global Step: 2500   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:36,375-Speed 80.81 samples/sec   Loss 39.5698   LearningRate 0.098661   Epoch: 0   Global Step: 2510   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:40,216-Speed 83.41 samples/sec   Loss 39.9659   LearningRate 0.098656   Epoch: 0   Global Step: 2520   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:44,140-Speed 81.55 samples/sec   Loss 39.2104   LearningRate 0.098651   Epoch: 0   Global Step: 2530   Fp16 Grad Scale: 4096   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:48,641-Speed 71.15 samples/sec   Loss 40.2999   LearningRate 0.098645   Epoch: 0   Global Step: 2540   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:52,637-Speed 80.27 samples/sec   Loss 39.5435   LearningRate 0.098640   Epoch: 0   Global Step: 2550   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:25:56,733-Speed 78.15 samples/sec   Loss 38.9988   LearningRate 0.098635   Epoch: 0   Global Step: 2560   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:26:01,150-Speed 72.46 samples/sec   Loss 39.7703   LearningRate 0.098629   Epoch: 0   Global Step: 2570   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:26:05,258-Speed 77.90 samples/sec   Loss 40.7838   LearningRate 0.098624   Epoch: 0   Global Step: 2580   Fp16 Grad Scale: 8192   Required: 28 hours\n",
      "Training: 2024-07-20 02:26:09,633-Speed 73.15 samples/sec   Loss 40.6846   LearningRate 0.098619   Epoch: 0   Global Step: 2590   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:26:14,362-Speed 67.68 samples/sec   Loss 39.6151   LearningRate 0.098613   Epoch: 0   Global Step: 2600   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:26:18,640-Speed 74.82 samples/sec   Loss 39.3934   LearningRate 0.098608   Epoch: 0   Global Step: 2610   Fp16 Grad Scale: 1024   Required: 28 hours\n",
      "Training: 2024-07-20 02:26:23,268-Speed 69.16 samples/sec   Loss 40.1844   LearningRate 0.098603   Epoch: 0   Global Step: 2620   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:27,176-Speed 82.02 samples/sec   Loss 39.3936   LearningRate 0.098597   Epoch: 0   Global Step: 2630   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:31,600-Speed 72.56 samples/sec   Loss 40.5431   LearningRate 0.098592   Epoch: 0   Global Step: 2640   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:35,615-Speed 79.73 samples/sec   Loss 39.4028   LearningRate 0.098587   Epoch: 0   Global Step: 2650   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:39,511-Speed 82.14 samples/sec   Loss 39.0042   LearningRate 0.098581   Epoch: 0   Global Step: 2660   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:43,862-Speed 73.68 samples/sec   Loss 39.1328   LearningRate 0.098576   Epoch: 0   Global Step: 2670   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:48,028-Speed 76.82 samples/sec   Loss 40.3949   LearningRate 0.098571   Epoch: 0   Global Step: 2680   Fp16 Grad Scale: 1024   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:52,233-Speed 76.11 samples/sec   Loss 42.8387   LearningRate 0.098565   Epoch: 0   Global Step: 2690   Fp16 Grad Scale: 2048   Required: 27 hours\n",
      "Training: 2024-07-20 02:26:56,446-Speed 76.07 samples/sec   Loss 39.6721   LearningRate 0.098560   Epoch: 0   Global Step: 2700   Fp16 Grad Scale: 2048   Required: 27 hours\n",
      "Training: 2024-07-20 02:27:00,451-Speed 79.92 samples/sec   Loss 40.1978   LearningRate 0.098555   Epoch: 0   Global Step: 2710   Fp16 Grad Scale: 2048   Required: 27 hours\n",
      "Training: 2024-07-20 02:27:05,119-Speed 68.57 samples/sec   Loss 40.2205   LearningRate 0.098549   Epoch: 0   Global Step: 2720   Fp16 Grad Scale: 2048   Required: 27 hours\n",
      "Training: 2024-07-20 02:27:14,190-Speed 35.28 samples/sec   Loss 42.0112   LearningRate 0.098544   Epoch: 0   Global Step: 2730   Fp16 Grad Scale: 2048   Required: 27 hours\n",
      "Training: 2024-07-20 02:27:46,524-Speed 9.90 samples/sec   Loss 40.5316   LearningRate 0.098539   Epoch: 0   Global Step: 2740   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:28:18,896-Speed 9.89 samples/sec   Loss 40.2990   LearningRate 0.098533   Epoch: 0   Global Step: 2750   Fp16 Grad Scale: 2048   Required: 28 hours\n",
      "Training: 2024-07-20 02:28:47,603-Speed 11.15 samples/sec   Loss 39.9188   LearningRate 0.098528   Epoch: 0   Global Step: 2760   Fp16 Grad Scale: 2048   Required: 29 hours\n",
      "Training: 2024-07-20 02:29:07,629-Speed 15.98 samples/sec   Loss 39.7268   LearningRate 0.098523   Epoch: 0   Global Step: 2770   Fp16 Grad Scale: 2048   Required: 29 hours\n",
      "Training: 2024-07-20 02:29:43,521-Speed 8.92 samples/sec   Loss 40.1835   LearningRate 0.098517   Epoch: 0   Global Step: 2780   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:30:09,417-Speed 12.36 samples/sec   Loss 40.9824   LearningRate 0.098512   Epoch: 0   Global Step: 2790   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:30:34,398-Speed 12.81 samples/sec   Loss 39.5904   LearningRate 0.098507   Epoch: 0   Global Step: 2800   Fp16 Grad Scale: 4096   Required: 30 hours\n",
      "Training: 2024-07-20 02:31:03,393-Speed 11.04 samples/sec   Loss 39.6263   LearningRate 0.098501   Epoch: 0   Global Step: 2810   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:17,592-Speed 22.54 samples/sec   Loss 39.9673   LearningRate 0.098496   Epoch: 0   Global Step: 2820   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:30,505-Speed 24.78 samples/sec   Loss 39.8705   LearningRate 0.098491   Epoch: 0   Global Step: 2830   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:36,835-Speed 50.56 samples/sec   Loss 39.1704   LearningRate 0.098485   Epoch: 0   Global Step: 2840   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:40,606-Speed 84.88 samples/sec   Loss 38.9185   LearningRate 0.098480   Epoch: 0   Global Step: 2850   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:44,387-Speed 84.76 samples/sec   Loss 39.9223   LearningRate 0.098475   Epoch: 0   Global Step: 2860   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:48,241-Speed 83.05 samples/sec   Loss 40.9356   LearningRate 0.098469   Epoch: 0   Global Step: 2870   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:52,089-Speed 83.18 samples/sec   Loss 39.6206   LearningRate 0.098464   Epoch: 0   Global Step: 2880   Fp16 Grad Scale: 4096   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:55,827-Speed 85.63 samples/sec   Loss 40.1911   LearningRate 0.098459   Epoch: 0   Global Step: 2890   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:31:59,737-Speed 81.85 samples/sec   Loss 39.5209   LearningRate 0.098453   Epoch: 0   Global Step: 2900   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:03,733-Speed 80.10 samples/sec   Loss 39.8100   LearningRate 0.098448   Epoch: 0   Global Step: 2910   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:07,920-Speed 76.42 samples/sec   Loss 39.5333   LearningRate 0.098443   Epoch: 0   Global Step: 2920   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:12,055-Speed 77.41 samples/sec   Loss 39.5227   LearningRate 0.098437   Epoch: 0   Global Step: 2930   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:16,249-Speed 76.31 samples/sec   Loss 39.9768   LearningRate 0.098432   Epoch: 0   Global Step: 2940   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:20,148-Speed 82.09 samples/sec   Loss 39.6513   LearningRate 0.098427   Epoch: 0   Global Step: 2950   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:24,168-Speed 79.60 samples/sec   Loss 39.3440   LearningRate 0.098421   Epoch: 0   Global Step: 2960   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:28,127-Speed 80.85 samples/sec   Loss 39.5132   LearningRate 0.098416   Epoch: 0   Global Step: 2970   Fp16 Grad Scale: 8192   Required: 31 hours\n",
      "Training: 2024-07-20 02:32:32,287-Speed 76.94 samples/sec   Loss 39.3730   LearningRate 0.098411   Epoch: 0   Global Step: 2980   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:32:36,709-Speed 72.39 samples/sec   Loss 39.9924   LearningRate 0.098405   Epoch: 0   Global Step: 2990   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:32:40,760-Speed 79.01 samples/sec   Loss 39.9801   LearningRate 0.098400   Epoch: 0   Global Step: 3000   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:32:44,635-Speed 82.59 samples/sec   Loss 39.4380   LearningRate 0.098395   Epoch: 0   Global Step: 3010   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:32:48,549-Speed 81.76 samples/sec   Loss 39.8302   LearningRate 0.098389   Epoch: 0   Global Step: 3020   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:32:52,825-Speed 74.85 samples/sec   Loss 39.2278   LearningRate 0.098384   Epoch: 0   Global Step: 3030   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:32:56,846-Speed 79.60 samples/sec   Loss 39.2658   LearningRate 0.098379   Epoch: 0   Global Step: 3040   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:01,232-Speed 72.96 samples/sec   Loss 39.5160   LearningRate 0.098373   Epoch: 0   Global Step: 3050   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:05,241-Speed 79.84 samples/sec   Loss 39.8597   LearningRate 0.098368   Epoch: 0   Global Step: 3060   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:09,639-Speed 72.77 samples/sec   Loss 39.7945   LearningRate 0.098363   Epoch: 0   Global Step: 3070   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:13,636-Speed 80.07 samples/sec   Loss 39.2613   LearningRate 0.098357   Epoch: 0   Global Step: 3080   Fp16 Grad Scale: 16384   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:17,905-Speed 75.07 samples/sec   Loss 39.3620   LearningRate 0.098352   Epoch: 0   Global Step: 3090   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:22,111-Speed 76.10 samples/sec   Loss 39.2517   LearningRate 0.098347   Epoch: 0   Global Step: 3100   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:26,056-Speed 81.52 samples/sec   Loss 39.5267   LearningRate 0.098341   Epoch: 0   Global Step: 3110   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:30,187-Speed 77.49 samples/sec   Loss 39.7051   LearningRate 0.098336   Epoch: 0   Global Step: 3120   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:34,639-Speed 71.89 samples/sec   Loss 39.8351   LearningRate 0.098331   Epoch: 0   Global Step: 3130   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:38,698-Speed 78.88 samples/sec   Loss 39.5712   LearningRate 0.098325   Epoch: 0   Global Step: 3140   Fp16 Grad Scale: 8192   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:42,631-Speed 81.57 samples/sec   Loss 40.8193   LearningRate 0.098320   Epoch: 0   Global Step: 3150   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:46,562-Speed 81.55 samples/sec   Loss 39.2292   LearningRate 0.098315   Epoch: 0   Global Step: 3160   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:50,925-Speed 73.77 samples/sec   Loss 39.6272   LearningRate 0.098309   Epoch: 0   Global Step: 3170   Fp16 Grad Scale: 2048   Required: 30 hours\n",
      "Training: 2024-07-20 02:33:54,833-Speed 81.90 samples/sec   Loss 39.4044   LearningRate 0.098304   Epoch: 0   Global Step: 3180   Fp16 Grad Scale: 1024   Required: 30 hours\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=12 train_v2.py configs/ghostfacenets_300k.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
